{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encodings\n",
    "\n",
    "Wie Ihr gerade gesehen habt erzeugen wir aus jedem Wort einen Vektor mit 512 Dimensionen.\n",
    "\n",
    "Da Wörter, bzw. deren vektorielle Repräsentation unterschiedliche Wahrnehmungen erzeugen, \n",
    "je nachdem welche Position diese Wörter im Satz einnehmen, möchten wir diese Position erfassen. \n",
    "\n",
    "Daher addieren wir zu den Embeddings (vektorielle Darstellung einer Sequenz) jeweils Vektoren mit der Länge 512. \n",
    "\n",
    "\n",
    "Diese Positionsvektoren sind immer gleich und folgen wie bereits erwähnt folgendem Schema: \n",
    "\n",
    "Für gerade Positionen:\n",
    "$$\n",
    "PE(pos, 2i) = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "Für ungerade Positionen:\n",
    "$$\n",
    "PE(pos, 2i + 1) = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "### Aufgabe 1\n",
    "\n",
    "Vervollständige __#1__ und __#2__. \n",
    "\n",
    "__Hinweis: Die Sinusfunktion in pytorch kannst du mit torch.sin() und die Cosinusfunktion mit torch.cos() aufrufen.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Erzeugung einer Nullmatrix pe mit den Dimensionen max_seq_length x d_model\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Erzeugung eines Tensor 'position' mit Werten von 0 bis max_seq_length-1 und Umwandlung in eine Spalte\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Berechnung des 'div_term' Tensors\n",
    "        div_term = torch.pow(10000.0, torch.arange(0, d_model, 2).float() / d_model)\n",
    "        \n",
    "        pe[:, 0::2] = #1\n",
    "        pe[:, 1::2] = #2\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    # Zuschnitt der Positional Encoding Matrix auf die Länge der Eingabe (Token Embeddings)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Wie Ihr gesehen habt wird das Embedding (Sequenzlänge × d_model) zunächst in Q, K und V aufgeteilt, wobei diese drei Matrizen identisch sind. \n",
    "Anschliessend werden diese mit einer lernbaren Gewichtsmatrix (linear Layer) (d_model × d_model) multipliziert.\n",
    "\n",
    "Zudem definieren wir noch eine Gewichtsmatrix derselben Größe für die zusammengefügte attention Matrix. \n",
    "\n",
    "### Aufgabe 2\n",
    "Vervollständige __#3__, __#4__, __#5__ und __#6__. \n",
    "\n",
    "__Hinweis: Linear Layers (lernbare Gewichtsmatrizen) werden in pytorch mit nn.Linear() erzeugt, wobei die Dimensionen übergeben werden.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Es ist notwendig, dass d_model durch num_heads teilbar ist.\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialisierung der Parameter\n",
    "        self.d_model = d_model # Modell dimension\n",
    "        self.num_heads = num_heads # Anzahl der Heads\n",
    "        self.d_k = d_model // num_heads # Dimension von jedem Head von K und Q und V\n",
    "        \n",
    "        # Lineare Transformationen für Q, K, V und Output\n",
    "        self.W_q = #3 # Query Gewichte \n",
    "        self.W_k = #4 # Key Gewichte\n",
    "        self.W_v = #5 # Value Gewichte\n",
    "        self.W_o = #6 # Output Gewichte\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Attention scores werden berechnet -> Dimension: SL*SL \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Optional: Maske wird angewendet\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax wird angewendet\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Attention wird berechnet in dem man eine Matrixmultiplikation mit V durchführt  -> Dimension: SL*d_k\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Aufteilen in Heads -> Dimension für jeden Head: SL*d_k\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Heads werden zusammengeführt -> Dimension: SL*d_model\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Lineare Transformationen für Q, K, V und aufteilen in Heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Attention wird berechnet\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Heads werden zusammengeführt und lineare Transformation für Output\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Layer\n",
    "\n",
    "Die Feed Forward Layer folgt folgendem Schema: \n",
    "\n",
    "$$\n",
    "FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "Es wird also der Relu-Output der ersten linearen Transformation mit der zweiten linearen Transformation multipliziert. \n",
    "### Aufgabe 3\n",
    "\n",
    "Vervollständige __#7__. \n",
    "\n",
    "__Hinweis: Um den Input für eine lineare Transformation in pytorch zu definieren kannst du auch den Output einer anderen linearen Transformation übergeben. Die ReLU kannst du anwenden indem du der Instanz eine Matrix übergibst__\n",
    "_______________________________________________________________________________________________________\n",
    "__Beispiel:__ \n",
    "\n",
    "self.Linear1(self.Linear2(x))\n",
    "\n",
    "Hier wird  Linear1 der Output von Linear2 übergeben. Vorrausgesetzt die Dimensionen stimmen.\n",
    "________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        # Lineare Transformation -> Dimension: d_model x d_ff\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        # Lineare Transformation -> Dimension: d_ff x d_model\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        # Relu Aktivierungsfunktion\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return #7\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau der Encoder Layer\n",
    "\n",
    "Wie das Schaubild zeigt besteht der Encoder aus der MultiHeadAttention Layer sowie einer FeedForward Layer und 2 Normalisierungen. \n",
    "\n",
    "Der folgende Code implementiert den Encoder. \n",
    "\n",
    "__Hinweis:__\n",
    "Der Output der MultiHeadAttention Layer sowie die Positional Encodings werden in der Normalization Layer übergeben. \n",
    "Das passiert hier mit einem '+'' obwohl man vielleicht ein ',' erwarten würde. \n",
    "Das liegt an einer sogenannten Residualverbindung.\n",
    "Ohne darauf genauer einzugehen handelt sich dabei um eine Technik um den Vanishing oder Exploding Gradient zu vermeiden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Die Komponenten des Encoders werden initialisiert\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Eine Dropout-Schicht wird hinzugefügt um u.A Overfitting zu vermeiden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Die Attention wird berechnet\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        # Erste Normalisierung\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # Feed Forward Netzwerk wird durchlaufen\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # Zweite Normalisierung\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau der Decoder Layer\n",
    "\n",
    "Analog zum Decoder. Die Schichten werden nacheinander implementiert. \n",
    "\n",
    "Zu unterscheiden ist hier zwischen der self-Attention und der cross-Attention. Während die self-attention im Decoder ähnlich zur self-Attention im Encoder funktioniert (bloß mit Maske), kombiniert die cross-Attention den Output des Encoders mit dem Output der self-Attention Layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Die Komponenten des Decoders werden initialisiert\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Das Herzsück des Decoders, die Cross-Attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # Eine Dropout-Schicht wird hinzugefügt um u.A Overfitting zu vermeiden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # Die Attention wird berechnet mit einer Maske\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        # Erste Normalisierung\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        # Die Cross-Attention wird berechnet\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        # Zweite Normalisierung\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        # Feed Forward Netzwerk wird durchlaufen\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # dritte Normalisierung\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau des Transformers\n",
    "\n",
    "__Das Modell wird hier zusammengebaut__ \n",
    "\n",
    "Hier muss nichts implementiert werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Definierung der Embeddings für Encoder und Decoder für jeden distinkten Token\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional Encoding wird definiert\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        # Initialisierung der n-Encoder- und n-Decoder-Layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Lineare Transformation um auf die Zielvokabulargröße zu projektieren\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Maske wird generiert\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # Embeddings werden erzeugt\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        # Encoder wird mit (Source) Input Embeddings und Maske durchlaufen\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Decoder wird mit (Target) Input Embeddings, Encoder Output und Maske durchlaufen\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Lineare Transformation wird durchgeführt um Dimension: SL x d_model -> SL x tgt_vocab_size zu projektieren\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training mit Dummy-Daten\n",
    "\n",
    "Um den Trainingsprozess zu veranschaulichen, trainieren wir hier das Transformer Modell mit zwei Sequenzen an zufälligen Zahlen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.695940017700195\n",
      "Epoch: 2, Loss: 8.565486907958984\n",
      "Epoch: 3, Loss: 8.493195533752441\n"
     ]
    }
   ],
   "source": [
    "# Parameter laut dem Paper \"Attention is All You Need\"\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "#sezen des seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generieren von Testdaten\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)) \n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "# Trainingsschleife\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausblick: Übersetzen mit Transformer Modellen\n",
    "\n",
    "Gerne dürft ihr den untenstehenden Code ausführen um eine Übersetzung durchzuführen. Der Prozess ist im Grunde derselbe wie oben.\n",
    "\n",
    "Die Übersetzung wird aufgrund der wenigen Daten natürlich (sehr) schlecht sein, \n",
    "man kann aber auch problemlos große Datensätze zum trainieren heranziehen. Falls man Lust hat, kann man das ja in seiner Freizeit mal probieren. \n",
    "\n",
    "Mit steigender Anzahl an Epochen steigt natürlich auch die Leistungsfähigkeit des Modells. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romanoelfken/miniconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/romanoelfken/miniconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/romanoelfken/miniconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.944688892364502\n",
      "Epoch: 2, Loss: 2.8693776448567707\n",
      "Epoch: 3, Loss: 1.735778268178304\n",
      "Epoch: 4, Loss: 1.1560726404190063\n",
      "Epoch: 5, Loss: 0.7502215166886648\n",
      "Epoch: 6, Loss: 0.6137418289979298\n",
      "Epoch: 7, Loss: 0.49692387382189435\n",
      "Epoch: 8, Loss: 0.46944422721862794\n",
      "Epoch: 9, Loss: 0.47423750956853233\n",
      "Epoch: 10, Loss: 0.3201397856076558\n",
      "Epoch: 11, Loss: 0.24799330482880275\n",
      "Epoch: 12, Loss: 0.16066059619188308\n",
      "Epoch: 13, Loss: 0.11286518834531307\n",
      "Epoch: 14, Loss: 0.0669271974513928\n",
      "Epoch: 15, Loss: 0.05244739806900422\n",
      "Epoch: 16, Loss: 0.05275492804745833\n",
      "Epoch: 17, Loss: 0.054446007745961346\n",
      "Epoch: 18, Loss: 0.024927052296698095\n",
      "Epoch: 19, Loss: 0.014441488745311896\n",
      "Epoch: 20, Loss: 0.011865517341842253\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "train_data = [\n",
    "    (\"Ich kann nicht glauben, dass das wahr ist.\", \"I can't believe this is true.\"),\n",
    "    (\"Das Wetter ist heute schön.\", \"The weather is nice today.\"),\n",
    "    (\"Ich liebe Programmierung.\", \"I love programming.\"),\n",
    "    (\"Wie geht es dir?\", \"How are you?\"),\n",
    "    (\"Das ist ein Test.\", \"This is a test.\"),\n",
    "    (\"Die Mensa macht immer gutes Essen.\", \"The canteen always makes good food.\"),\n",
    "    (\"Ich bin hungrig.\", \"I am hungry.\"),\n",
    "    (\"Was machst du?\", \"What are you doing?\"),\n",
    "    (\"Es ist ein schöner Tag.\", \"It is a beautiful day.\"),\n",
    "    (\"Das Essen schmeckt schlecht.\", \"The food tastes bad.\"),\n",
    "    (\"Das Essen schmeckt gut.\", \"The food tastes good.\"),\n",
    "    (\"Ich gehe zur Schule.\", \"I am going to school.\"),\n",
    "    (\"Was hast du heute gelernt?\", \"What did you learn today?\"),\n",
    "    (\"Ich habe eine Katze.\", \"I have a cat.\"),\n",
    "    (\"Ich habe einen Hund.\", \"I have a dog.\")\n",
    "]\n",
    "\n",
    "src_tokenizer = get_tokenizer('basic_english')\n",
    "tgt_tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "def yield_tokens(data, tokenizer):\n",
    "    for src_sentence, tgt_sentence in data:\n",
    "        yield tokenizer(src_sentence)\n",
    "        yield tokenizer(tgt_sentence)\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(train_data, src_tokenizer), specials=['<pad>', '<sos>', '<eos>'])\n",
    "tgt_vocab = build_vocab_from_iterator(yield_tokens(train_data, tgt_tokenizer), specials=['<pad>', '<sos>', '<eos>'])\n",
    "\n",
    "src_vocab.set_default_index(src_vocab['<pad>'])\n",
    "tgt_vocab.set_default_index(tgt_vocab['<pad>'])\n",
    "\n",
    "def sentence_to_tensor(sentence, vocab, tokenizer):\n",
    "    tokens = tokenizer(sentence)\n",
    "    indices = [vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']]\n",
    "    return torch.tensor(indices).unsqueeze(0)\n",
    "\n",
    "train_src_tensors = [sentence_to_tensor(src, src_vocab, src_tokenizer) for src, tgt in train_data]\n",
    "train_tgt_tensors = [sentence_to_tensor(tgt, tgt_vocab, tgt_tokenizer) for src, tgt in train_data]\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<pad>'])\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(20): \n",
    "    total_loss = 0\n",
    "    for src_tensor, tgt_tensor in zip(train_src_tensors, train_tgt_tensors):\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(src_tensor, tgt_tensor[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_tensor[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "    \n",
    "\n",
    "def translate(transformer, src_sentence, src_vocab, tgt_vocab, max_seq_length):\n",
    "    transformer.eval()\n",
    "    \n",
    "    src_tokens = src_tokenizer(src_sentence)\n",
    "    src_indices = [src_vocab['<sos>']] + [src_vocab[token] for token in src_tokens] + [src_vocab['<eos>']]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0)\n",
    "    \n",
    "    tgt_indices = [tgt_vocab['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_length):\n",
    "            output = transformer(src_tensor, tgt_tensor)\n",
    "            next_token = output.argmax(2)[:, -1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            if next_token == tgt_vocab['<eos>']:\n",
    "                break\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)\n",
    "    \n",
    "    tgt_tokens = [tgt_vocab.get_itos()[idx] for idx in tgt_indices]\n",
    "    return ' '.join(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: <sos> i have a dog . <eos>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Ich habe einen Hund\"\n",
    "translated_sentence = translate(transformer, test_sentence, src_vocab, tgt_vocab, max_seq_length)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
