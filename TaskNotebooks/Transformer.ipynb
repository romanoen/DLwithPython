{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encodings\n",
    "\n",
    "Wie Ihr gerade gesehen habt erzeugen wir aus jedem Wort einen Vektor mit 512 Dimensionen.\n",
    "\n",
    "Da Wörter, bzw. deren vektorielle Repräsentation unterschiedliche Wahrnehmungen erzeugen, \n",
    "je nachdem welche Position diese Wörter im Satz einnehmen, möchten wir diese Position erfassen. \n",
    "\n",
    "Daher addieren wir zu den Embeddings (vektorielle Darstellung einer Sequenz) jeweils Vektoren mit der Länge 512. \n",
    "\n",
    "\n",
    "Diese Positionsvektoren sind immer gleich und folgen wie bereits erwähnt folgendem Schema: \n",
    "\n",
    "Für gerade Positionen:\n",
    "$$\n",
    "PE(pos, 2i) = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "Für ungerade Positionen:\n",
    "$$\n",
    "PE(pos, 2i + 1) = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)\n",
    "$$\n",
    "\n",
    "### Aufgabe 1\n",
    "\n",
    "Vervollständige __#1__ und __#2__. \n",
    "\n",
    "__Hinweis: Die Sinusfunktion in pytorch kannst du mit torch.sin() und die Cosinusfunktion mit torch.cos() aufrufen.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = #1\n",
    "        pe[:, 1::2] = #2\n",
    "        \n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Wie Ihr gesehen habt wird das Embedding (Sequenzlänge × d_model) zunächst in Q, K und V aufgeteilt, wobei diese drei Matrizen identisch sind. \n",
    "Anschliessend werden diese mit einer lernbaren Gewichtsmatrix (linear Layer) (d_model × d_model) multipliziert.\n",
    "\n",
    "Zudem definieren wir noch eine Gewichtsmatrix derselben Größe für die zusammengefügte attention Matrix. \n",
    "\n",
    "### Aufgabe 2\n",
    "Vervollständige __#3__, __#4__, __#5__ und __#6__. \n",
    "\n",
    "__Hinweis: Linear Layers (lernbare Gewichtsmatrizen) werden in pytorch mit nn.Linear() erzeugt, wobei die Dimensionen übergeben werden.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = #3 # Query transformation\n",
    "        self.W_k = #4 # Key transformation\n",
    "        self.W_v = #5 # Value transformation\n",
    "        self.W_o = #6 # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Layer\n",
    "\n",
    "Die Feed Forward Layer folgt folgendem Schema: \n",
    "\n",
    "$$\n",
    "FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "Es wird also der Relu-Output der ersten lineare Transformation mit der zweiten linearen Transformation multipliziert. \n",
    "### Aufgabe 3\n",
    "\n",
    "Vervollständige __#7__. \n",
    "\n",
    "__Hinweis: Um den Input für eine lineare Transformation in pytorch zu definieren kannst du den Output einer anderen linearen Transformation übergeben.__\n",
    "\n",
    "__Beispiel:__ \n",
    "\n",
    "Linear1(Linear2)\n",
    "\n",
    "Hier wird der Linearen Schicht Linear1 der Output der Schicht Linear2 übergeben. Vorrausgesetzt die Dimensionen stimmen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return #7\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau der Encoder Layer\n",
    "\n",
    "Wie das Schaubild zeigt besteht der Encoder aus der MultiHeadAttention Layer sowie einer FeedForward Layer und 2 Normalisierungen. \n",
    "\n",
    "Der folgende Code implementiert den Encoder. \n",
    "\n",
    "__Hinweis:__\n",
    "Der Output der MultiHeadAttention Layer sowie die Positional Encodings werden in der Normalization Layer übergeben. \n",
    "Das passiert hier mit einem '+'' obwohl man vielleicht ein ',' erwarten würde. \n",
    "Das liegt an einer sogenannten Residualverbindung.\n",
    "Ohne darauf genauer einzugehen handelt sich dabei um eine Technik um den Vanishing oder Exploding Gradient zu vermeiden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau der Decoder Layer\n",
    "\n",
    "Analog zum Decoder. Die Schichten werden nacheinander implementiert. \n",
    "\n",
    "Zu unterscheiden ist hier zwischen der self-Attention und der cross-Attention. Während die self-attention im Decoder ähnlich zur self-Attention im Encoder funktioniert (bloß mit Maske), kombiniert die cross-Attention den Output des Encoders mit dem Output der self-Attention Layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zusammenbau des Transformers\n",
    "\n",
    "__Das Modell wird hier zusammengebaut__ \n",
    "\n",
    "Hier muss nichts implementiert werden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training mit Dummy-Daten\n",
    "\n",
    "Um den Trainingsprozess zu veranschaulichen, trainieren wir hier das Transformer Modell mit zwei Sequenzen an zufälligen Zahlen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.67752742767334\n",
      "Epoch: 2, Loss: 8.548323631286621\n",
      "Epoch: 3, Loss: 8.483274459838867\n",
      "Epoch: 4, Loss: 8.426873207092285\n",
      "Epoch: 5, Loss: 8.36446762084961\n",
      "Epoch: 6, Loss: 8.295404434204102\n",
      "Epoch: 7, Loss: 8.215231895446777\n",
      "Epoch: 8, Loss: 8.132074356079102\n",
      "Epoch: 9, Loss: 8.052645683288574\n",
      "Epoch: 10, Loss: 7.967570781707764\n",
      "Epoch: 11, Loss: 7.8843770027160645\n",
      "Epoch: 12, Loss: 7.803915500640869\n",
      "Epoch: 13, Loss: 7.71641731262207\n",
      "Epoch: 14, Loss: 7.6328349113464355\n",
      "Epoch: 15, Loss: 7.552999019622803\n",
      "Epoch: 16, Loss: 7.470870018005371\n",
      "Epoch: 17, Loss: 7.388860702514648\n",
      "Epoch: 18, Loss: 7.305726528167725\n",
      "Epoch: 19, Loss: 7.224644660949707\n",
      "Epoch: 20, Loss: 7.1422553062438965\n",
      "Epoch: 21, Loss: 7.05832052230835\n",
      "Epoch: 22, Loss: 6.984397888183594\n",
      "Epoch: 23, Loss: 6.9108357429504395\n",
      "Epoch: 24, Loss: 6.824675559997559\n",
      "Epoch: 25, Loss: 6.750906467437744\n",
      "Epoch: 26, Loss: 6.676790714263916\n",
      "Epoch: 27, Loss: 6.607114315032959\n",
      "Epoch: 28, Loss: 6.532591342926025\n",
      "Epoch: 29, Loss: 6.466593265533447\n",
      "Epoch: 30, Loss: 6.395503997802734\n",
      "Epoch: 31, Loss: 6.324738025665283\n",
      "Epoch: 32, Loss: 6.253464221954346\n",
      "Epoch: 33, Loss: 6.19133186340332\n",
      "Epoch: 34, Loss: 6.1198601722717285\n",
      "Epoch: 35, Loss: 6.051551342010498\n",
      "Epoch: 36, Loss: 5.9866437911987305\n",
      "Epoch: 37, Loss: 5.922215461730957\n",
      "Epoch: 38, Loss: 5.855256080627441\n",
      "Epoch: 39, Loss: 5.7969231605529785\n",
      "Epoch: 40, Loss: 5.735703468322754\n",
      "Epoch: 41, Loss: 5.671273708343506\n",
      "Epoch: 42, Loss: 5.6095194816589355\n",
      "Epoch: 43, Loss: 5.5445556640625\n",
      "Epoch: 44, Loss: 5.487635135650635\n",
      "Epoch: 45, Loss: 5.427639007568359\n",
      "Epoch: 46, Loss: 5.367010593414307\n",
      "Epoch: 47, Loss: 5.306920528411865\n",
      "Epoch: 48, Loss: 5.252767562866211\n",
      "Epoch: 49, Loss: 5.193630218505859\n",
      "Epoch: 50, Loss: 5.133896827697754\n",
      "Epoch: 51, Loss: 5.087989330291748\n",
      "Epoch: 52, Loss: 5.026175022125244\n",
      "Epoch: 53, Loss: 4.966396331787109\n",
      "Epoch: 54, Loss: 4.912449359893799\n",
      "Epoch: 55, Loss: 4.853301525115967\n",
      "Epoch: 56, Loss: 4.804861068725586\n",
      "Epoch: 57, Loss: 4.750809669494629\n",
      "Epoch: 58, Loss: 4.704639911651611\n",
      "Epoch: 59, Loss: 4.649435043334961\n",
      "Epoch: 60, Loss: 4.590197563171387\n",
      "Epoch: 61, Loss: 4.545070648193359\n",
      "Epoch: 62, Loss: 4.492167949676514\n",
      "Epoch: 63, Loss: 4.440932750701904\n",
      "Epoch: 64, Loss: 4.3814802169799805\n",
      "Epoch: 65, Loss: 4.34212589263916\n",
      "Epoch: 66, Loss: 4.291750431060791\n",
      "Epoch: 67, Loss: 4.237366199493408\n",
      "Epoch: 68, Loss: 4.187119960784912\n",
      "Epoch: 69, Loss: 4.139854431152344\n",
      "Epoch: 70, Loss: 4.088116645812988\n",
      "Epoch: 71, Loss: 4.038862228393555\n",
      "Epoch: 72, Loss: 3.9979541301727295\n",
      "Epoch: 73, Loss: 3.9433412551879883\n",
      "Epoch: 74, Loss: 3.8924338817596436\n",
      "Epoch: 75, Loss: 3.847181558609009\n",
      "Epoch: 76, Loss: 3.797027587890625\n",
      "Epoch: 77, Loss: 3.750264883041382\n",
      "Epoch: 78, Loss: 3.7061469554901123\n",
      "Epoch: 79, Loss: 3.6592295169830322\n",
      "Epoch: 80, Loss: 3.6094095706939697\n",
      "Epoch: 81, Loss: 3.5614054203033447\n",
      "Epoch: 82, Loss: 3.51674747467041\n",
      "Epoch: 83, Loss: 3.466437339782715\n",
      "Epoch: 84, Loss: 3.4221131801605225\n",
      "Epoch: 85, Loss: 3.3842995166778564\n",
      "Epoch: 86, Loss: 3.335574150085449\n",
      "Epoch: 87, Loss: 3.29937744140625\n",
      "Epoch: 88, Loss: 3.246269702911377\n",
      "Epoch: 89, Loss: 3.195237636566162\n",
      "Epoch: 90, Loss: 3.1532669067382812\n",
      "Epoch: 91, Loss: 3.1085119247436523\n",
      "Epoch: 92, Loss: 3.070983648300171\n",
      "Epoch: 93, Loss: 3.0267765522003174\n",
      "Epoch: 94, Loss: 2.9816548824310303\n",
      "Epoch: 95, Loss: 2.937782049179077\n",
      "Epoch: 96, Loss: 2.9009971618652344\n",
      "Epoch: 97, Loss: 2.8531880378723145\n",
      "Epoch: 98, Loss: 2.812129497528076\n",
      "Epoch: 99, Loss: 2.7700419425964355\n",
      "Epoch: 100, Loss: 2.7312166690826416\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "#set seed\n",
    "torch.manual_seed(42)\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)) \n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
    "src_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausblick: Übersetzen mit Transformer Modellen\n",
    "\n",
    "Gerne dürft ihr den untenstehenden Code ausführen um eine Übersetzung durchzuführen. Der Prozess ist im Grunde derselbe wie oben.\n",
    "\n",
    "Die Übersetzung wird aufgrund der wenigen Daten natürlich (sehr) schlecht sein, \n",
    "man kann aber auch problemlos große Datensätze zum trainieren heranziehen. Falls man Lust hat, kann man das ja in seiner Freizeit mal probieren. \n",
    "\n",
    "Mit steigender Anzahl an Epochen steigt natürlich auch die Leistungsfähigkeit des Modells. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 4.017286030451457\n",
      "Epoch: 2, Loss: 2.861263378461202\n",
      "Epoch: 3, Loss: 1.725689705212911\n",
      "Epoch: 4, Loss: 1.1149355173110962\n",
      "Epoch: 5, Loss: 0.7745535929997762\n",
      "Epoch: 6, Loss: 0.5794821302096049\n",
      "Epoch: 7, Loss: 0.416988006234169\n",
      "Epoch: 8, Loss: 0.3256495401263237\n",
      "Epoch: 9, Loss: 0.3196621763209502\n",
      "Epoch: 10, Loss: 0.2375067062675953\n",
      "Epoch: 11, Loss: 0.21858055517077446\n",
      "Epoch: 12, Loss: 0.20526507397492727\n",
      "Epoch: 13, Loss: 0.08913759837547938\n",
      "Epoch: 14, Loss: 0.050353662483394145\n",
      "Epoch: 15, Loss: 0.0762476397678256\n",
      "Epoch: 16, Loss: 0.16411986568321785\n",
      "Epoch: 17, Loss: 0.15593199878931047\n",
      "Epoch: 18, Loss: 0.23985166649023693\n",
      "Epoch: 19, Loss: 0.1940862928206722\n",
      "Epoch: 20, Loss: 0.17035357523709535\n",
      "Epoch: 21, Loss: 0.23044233818848928\n",
      "Epoch: 22, Loss: 0.0744379414866368\n",
      "Epoch: 23, Loss: 0.03281161999329925\n",
      "Epoch: 24, Loss: 0.025955278240144253\n",
      "Epoch: 25, Loss: 0.01208933675661683\n",
      "Epoch: 26, Loss: 0.008737101343770821\n",
      "Epoch: 27, Loss: 0.004833026199291149\n",
      "Epoch: 28, Loss: 0.003958126250654459\n",
      "Epoch: 29, Loss: 0.003316187672317028\n",
      "Epoch: 30, Loss: 0.002910046000033617\n",
      "Epoch: 31, Loss: 0.002680160291492939\n",
      "Epoch: 32, Loss: 0.002332375959182779\n",
      "Epoch: 33, Loss: 0.002123043267056346\n",
      "Epoch: 34, Loss: 0.0020129957391570013\n",
      "Epoch: 35, Loss: 0.0018013300839811563\n",
      "Epoch: 36, Loss: 0.0016139128245413303\n",
      "Epoch: 37, Loss: 0.0015005409523534277\n",
      "Epoch: 38, Loss: 0.0013432945551661154\n",
      "Epoch: 39, Loss: 0.0012584672309458255\n",
      "Epoch: 40, Loss: 0.0011205684820500513\n",
      "Epoch: 41, Loss: 0.0009992224241917333\n",
      "Epoch: 42, Loss: 0.0008958948077633977\n",
      "Epoch: 43, Loss: 0.0007773302825322995\n",
      "Epoch: 44, Loss: 0.0007231522099270175\n",
      "Epoch: 45, Loss: 0.0006473485205788166\n",
      "Epoch: 46, Loss: 0.0005878908191031466\n",
      "Epoch: 47, Loss: 0.0005128669901750982\n",
      "Epoch: 48, Loss: 0.0004728979625118276\n",
      "Epoch: 49, Loss: 0.00042707224880966045\n",
      "Epoch: 50, Loss: 0.0003823389803680281\n",
      "Epoch: 51, Loss: 0.00033531593023023256\n",
      "Epoch: 52, Loss: 0.0003167984425090253\n",
      "Epoch: 53, Loss: 0.00027143850456923246\n",
      "Epoch: 54, Loss: 0.0002481979870935902\n",
      "Epoch: 55, Loss: 0.00021393468972140303\n",
      "Epoch: 56, Loss: 0.0001944182023483639\n",
      "Epoch: 57, Loss: 0.0001728917850414291\n",
      "Epoch: 58, Loss: 0.00015521289945657674\n",
      "Epoch: 59, Loss: 0.00013788554448789607\n",
      "Epoch: 60, Loss: 0.00012375431785282369\n",
      "Epoch: 61, Loss: 0.00010941710740250224\n",
      "Epoch: 62, Loss: 9.70801236690022e-05\n",
      "Epoch: 63, Loss: 8.704850309489605e-05\n",
      "Epoch: 64, Loss: 7.864304740602771e-05\n",
      "Epoch: 65, Loss: 7.004813523963094e-05\n",
      "Epoch: 66, Loss: 6.549862422010241e-05\n",
      "Epoch: 67, Loss: 5.802099500821593e-05\n",
      "Epoch: 68, Loss: 5.097568185495523e-05\n",
      "Epoch: 69, Loss: 4.483093168043221e-05\n",
      "Epoch: 70, Loss: 4.018564407791321e-05\n",
      "Epoch: 71, Loss: 3.799179564036119e-05\n",
      "Epoch: 72, Loss: 3.2118787324482885e-05\n",
      "Epoch: 73, Loss: 2.8656418241250018e-05\n",
      "Epoch: 74, Loss: 2.5660940385326587e-05\n",
      "Epoch: 75, Loss: 2.3094217067409772e-05\n",
      "Epoch: 76, Loss: 2.0645429564562316e-05\n",
      "Epoch: 77, Loss: 2.010004627663875e-05\n",
      "Epoch: 78, Loss: 1.8682904252879475e-05\n",
      "Epoch: 79, Loss: 1.547918418509653e-05\n",
      "Epoch: 80, Loss: 1.3908421139300724e-05\n",
      "Epoch: 81, Loss: 1.1533263265543307e-05\n",
      "Epoch: 82, Loss: 1.062028544159451e-05\n",
      "Epoch: 83, Loss: 9.494264971484274e-06\n",
      "Epoch: 84, Loss: 8.685111576293517e-06\n",
      "Epoch: 85, Loss: 7.868898380062699e-06\n",
      "Epoch: 86, Loss: 7.76999947144456e-06\n",
      "Epoch: 87, Loss: 7.545320052789369e-06\n",
      "Epoch: 88, Loss: 0.0013754364201607435\n",
      "Epoch: 89, Loss: 0.5069570143488817\n",
      "Epoch: 90, Loss: 0.5196151457843371\n",
      "Epoch: 91, Loss: 0.19873735591148337\n",
      "Epoch: 92, Loss: 0.09420682423127194\n",
      "Epoch: 93, Loss: 0.15275613994454035\n",
      "Epoch: 94, Loss: 0.030549026342729727\n",
      "Epoch: 95, Loss: 0.005317343796195928\n",
      "Epoch: 96, Loss: 0.002239872129575815\n",
      "Epoch: 97, Loss: 0.0003824140044647114\n",
      "Epoch: 98, Loss: 0.00035353296480025167\n",
      "Epoch: 99, Loss: 0.00034052806889424875\n",
      "Epoch: 100, Loss: 0.00026785334800176014\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "train_data = [\n",
    "    (\"Ich kann nicht glauben, dass das wahr ist.\", \"I can't believe this is true.\"),\n",
    "    (\"Das Wetter ist heute schön.\", \"The weather is nice today.\"),\n",
    "    (\"Ich liebe Programmierung.\", \"I love programming.\"),\n",
    "    (\"Wie geht es dir?\", \"How are you?\"),\n",
    "    (\"Das ist ein Test.\", \"This is a test.\"),\n",
    "    (\"Die Mensa macht immer gutes Essen.\", \"The canteen always makes good food.\"),\n",
    "    (\"Ich bin hungrig.\", \"I am hungry.\"),\n",
    "    (\"Was machst du?\", \"What are you doing?\"),\n",
    "    (\"Es ist ein schöner Tag.\", \"It is a beautiful day.\"),\n",
    "    (\"Das Essen schmeckt schlecht.\", \"The food tastes bad.\"),\n",
    "    (\"Das Essen schmeckt gut.\", \"The food tastes good.\"),\n",
    "    (\"Ich gehe zur Schule.\", \"I am going to school.\"),\n",
    "    (\"Was hast du heute gelernt?\", \"What did you learn today?\"),\n",
    "    (\"Ich habe eine Katze.\", \"I have a cat.\"),\n",
    "    (\"Ich habe einen Hund.\", \"I have a dog.\")\n",
    "]\n",
    "\n",
    "src_tokenizer = get_tokenizer('basic_english')\n",
    "tgt_tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "\n",
    "def yield_tokens(data, tokenizer):\n",
    "    for src_sentence, tgt_sentence in data:\n",
    "        yield tokenizer(src_sentence)\n",
    "        yield tokenizer(tgt_sentence)\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(train_data, src_tokenizer), specials=['<pad>', '<sos>', '<eos>'])\n",
    "tgt_vocab = build_vocab_from_iterator(yield_tokens(train_data, tgt_tokenizer), specials=['<pad>', '<sos>', '<eos>'])\n",
    "\n",
    "src_vocab.set_default_index(src_vocab['<pad>'])\n",
    "tgt_vocab.set_default_index(tgt_vocab['<pad>'])\n",
    "\n",
    "def sentence_to_tensor(sentence, vocab, tokenizer):\n",
    "    tokens = tokenizer(sentence)\n",
    "    indices = [vocab['<sos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']]\n",
    "    return torch.tensor(indices).unsqueeze(0)\n",
    "\n",
    "train_src_tensors = [sentence_to_tensor(src, src_vocab, src_tokenizer) for src, tgt in train_data]\n",
    "train_tgt_tensors = [sentence_to_tensor(tgt, tgt_vocab, tgt_tokenizer) for src, tgt in train_data]\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<pad>'])\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100): \n",
    "    total_loss = 0\n",
    "    for src_tensor, tgt_tensor in zip(train_src_tensors, train_tgt_tensors):\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(src_tensor, tgt_tensor[:, :-1])\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_tensor[:, 1:].contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "    \n",
    "\n",
    "def translate(transformer, src_sentence, src_vocab, tgt_vocab, max_seq_length):\n",
    "    transformer.eval()\n",
    "    \n",
    "    src_tokens = src_tokenizer(src_sentence)\n",
    "    src_indices = [src_vocab['<sos>']] + [src_vocab[token] for token in src_tokens] + [src_vocab['<eos>']]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0)\n",
    "    \n",
    "    tgt_indices = [tgt_vocab['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_seq_length):\n",
    "            output = transformer(src_tensor, tgt_tensor)\n",
    "            next_token = output.argmax(2)[:, -1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            if next_token == tgt_vocab['<eos>']:\n",
    "                break\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0)\n",
    "    \n",
    "    tgt_tokens = [tgt_vocab.get_itos()[idx] for idx in tgt_indices]\n",
    "    return ' '.join(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: <sos> i have a dog . <eos>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Ich habe ein Hund.\"\n",
    "translated_sentence = translate(transformer, test_sentence, src_vocab, tgt_vocab, max_seq_length)\n",
    "print(f\"Translated Sentence: {translated_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
